{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415caf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CUDA\n",
      "\n",
      "[Theoretical graph/activation stash (fp32)]\n",
      "Forward-mode (manual, with u_xx & u_yy): ~0.230 MB\n",
      "autograd.grad  (low..up bounds):        ~0.138 .. 0.230 MB\n",
      "[Manual forward-mode (u_xx, u_yy)] Peak CUDA allocated: 16.816 MB\n",
      "[autograd.grad (u_xx, u_yy)] Peak CUDA allocated: 16.790 MB\n",
      "\n",
      "Loss (manual):   0.866808\n",
      "Loss (autograd): 0.619902\n",
      "\n",
      "Note: CUDA numbers are peak *allocated* VRAM in the step,\n",
      "      dominated by 'saved for backward' activations + grad graphs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "N_PDE = 100          # collocation (interior) points\n",
    "N_BC_EDGE = 100      # boundary samples per edge\n",
    "HIDDEN = [20, 20, 20]\n",
    "DTYPE = torch.float32\n",
    "SEED = 0\n",
    "LR = 1e-3\n",
    "\n",
    "# ------------------------------\n",
    "# Utils\n",
    "# ------------------------------\n",
    "def set_seed(seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def bytes_to_mb(nbytes: int) -> float:\n",
    "    return nbytes / (1024**2)\n",
    "\n",
    "def on_cuda() -> bool:\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def device():\n",
    "    return torch.device(\"cuda\") if on_cuda() else torch.device(\"cpu\")\n",
    "\n",
    "# ------------------------------\n",
    "# Model\n",
    "# ------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden: List[int] = [20,20,20], out_dim=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(last, h))\n",
    "            last = h\n",
    "        self.hidden = nn.ModuleList(layers)\n",
    "        self.out = nn.Linear(last, out_dim)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "        # (optional) nice init\n",
    "        for lin in self.hidden:\n",
    "            nn.init.xavier_normal_(lin.weight, gain=1.0)\n",
    "            nn.init.zeros_(lin.bias)\n",
    "        nn.init.xavier_normal_(self.out.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.out.bias)\n",
    "\n",
    "    def forward(self, z0):  # z0: (N,2) with columns [x,y]\n",
    "        z = z0\n",
    "        Ls, Zs = [], []\n",
    "        for lin in self.hidden:\n",
    "            # Pre-activation: L = Z @ W^T + b    (shapes: (N,last)*(h,last)^T => (N,h))\n",
    "            L = z @ lin.weight.t() + lin.bias\n",
    "            z = self.act(L)\n",
    "            Ls.append(L)\n",
    "            Zs.append(z)\n",
    "        # Output (linear): u = Z @ W_out^T + b_out  => (N,1)\n",
    "        u = z @ self.out.weight.t() + self.out.bias\n",
    "        return u, Ls, Zs\n",
    "\n",
    "# ------------------------------\n",
    "# Manual forward-mode derivatives\n",
    "# ------------------------------\n",
    "def tanh_prime_from_Z(Z):\n",
    "    # σ'(a) = 1 - tanh(a)^2 ; since Z=tanh(a), σ'(a)=1 - Z^2\n",
    "    return 1.0 - Z**2\n",
    "\n",
    "def tanh_doubleprime_from_L_and_Z(L, Z):\n",
    "    # σ''(a) = -2 * tanh(a) * (1 - tanh(a)^2) = -2 * Z * (1 - Z^2)\n",
    "    return -2.0 * Z * (1.0 - Z**2)\n",
    "\n",
    "@torch.enable_grad()   # ensure params get grads through manual ops\n",
    "def manual_seconds(model: MLP, z0: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute u, u_xx, u_yy with forward-mode streams (no autograd.grad on inputs).\n",
    "    z0: (N,2) with columns [x,y]. requires_grad not needed here.\n",
    "    \"\"\"\n",
    "    # Forward pass (cache Ls, Zs)\n",
    "    u, Ls, Zs = model(z0)\n",
    "    # Weight matrices to use: for derivative recurrences we need W_i^T (same as forward)\n",
    "    WTs = [lin.weight for lin in model.hidden]  # (out,in); we'll use .t() when multiplying\n",
    "    W_out_T = model.out.weight  # (1, last)\n",
    "\n",
    "    N = z0.shape[0]\n",
    "    widths = [lin.out_features for lin in model.hidden]\n",
    "\n",
    "    # Base derivative streams for x and y at input:\n",
    "    # G0 has shape (N, in_dim=2); for ∂/∂x it's [1,0], for ∂/∂y it's [0,1].\n",
    "    G0x = torch.zeros_like(z0); G0x[:, 0] = 1.0\n",
    "    G0y = torch.zeros_like(z0); G0y[:, 1] = 1.0\n",
    "\n",
    "    # First-order streams G (post-activation) and H (post-linear)\n",
    "    # Second-order streams F (post-linear) and E (post-activation)\n",
    "    # We'll carry \"current\" G and F as we traverse layers; H,E are per-layer temporaries.\n",
    "    def streams_for_coord(G0):\n",
    "        G_prev = G0                     # (N, in_dim or width of previous)\n",
    "        F_prev = None                   # None means zeros at first layer\n",
    "        for i, (L, Z, W) in enumerate(zip(Ls, Zs, WTs)):\n",
    "            # Through linear: H_i = G_{i-1} @ W_i^T  where W_i is (out,in)\n",
    "            H_i = G_prev @ W.t()        # (N, width_i)\n",
    "            # First derivative through activation:\n",
    "            sig1 = tanh_prime_from_Z(Z)     # (N, width_i)\n",
    "            G_i = sig1 * H_i\n",
    "            # Second derivative:\n",
    "            sig2 = tanh_doubleprime_from_L_and_Z(L, Z)  # (N, width_i)\n",
    "            if F_prev is None:\n",
    "                F_prev = torch.zeros_like(G_i)          # F_1 = 0 (N, width_1)\n",
    "            C_i = sig2 * (H_i * H_i)                    # (N, width_i)\n",
    "            E_i = C_i + sig1 * F_prev                   # (N, width_i)\n",
    "            # Prepare for next layer: F_{i+1} = E_i @ W_{i+1}^T (if there is a next hidden layer)\n",
    "            F_next = None\n",
    "            if i < len(Ls) - 1:\n",
    "                W_next = WTs[i+1]\n",
    "                F_next = E_i @ W_next.t()               # (N, width_{i+1})\n",
    "            # roll\n",
    "            G_prev = G_i\n",
    "            F_prev = F_next if F_next is not None else F_prev\n",
    "            # Cache last layer's E, G for output derivatives\n",
    "            if i == len(Ls) - 1:\n",
    "                E_last = E_i\n",
    "                G_last = G_i\n",
    "        # Output derivatives (linear head):\n",
    "        # u_c  = G_last @ W_out^T   ; u_cc = E_last @ W_out^T\n",
    "        u_c  = G_last @ W_out_T.t()   # (N,1)\n",
    "        u_cc = E_last @ W_out_T.t()   # (N,1)\n",
    "        return u_c, u_cc\n",
    "\n",
    "    ux, uxx = streams_for_coord(G0x)   # (N,1), (N,1)\n",
    "    uy, uyy = streams_for_coord(G0y)   # (N,1), (N,1)\n",
    "    return u, uxx, uyy  # (N,1) each\n",
    "\n",
    "# ------------------------------\n",
    "# autograd.grad-based seconds\n",
    "# ------------------------------\n",
    "def autograd_seconds(model: MLP, x: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute u, u_xx, u_yy using autograd.grad with create_graph=True.\n",
    "    x,y must have requires_grad=True.\n",
    "    \"\"\"\n",
    "    z0 = torch.cat([x, y], dim=1)            # (N,2)\n",
    "    u, _, _ = model(z0)                      # (N,1)\n",
    "\n",
    "    ones = torch.ones_like(u)\n",
    "\n",
    "    # First derivatives: retain_graph because we'll also get grads w.r.t. the *other* input\n",
    "    ux = torch.autograd.grad(u, x, grad_outputs=ones, create_graph=True, retain_graph=True)[0]   # (N,1)\n",
    "    uy = torch.autograd.grad(u, y, grad_outputs=ones, create_graph=True, retain_graph=True)[0]   # (N,1)\n",
    "\n",
    "    # Second derivatives:\n",
    "    uxx = torch.autograd.grad(ux, x, grad_outputs=torch.ones_like(ux), create_graph=True)[0]     # (N,1)\n",
    "    uyy = torch.autograd.grad(uy, y, grad_outputs=torch.ones_like(uy), create_graph=True)[0]     # (N,1)\n",
    "\n",
    "    return u, uxx, uyy\n",
    "\n",
    "# ------------------------------\n",
    "# Data (PDE + boundary samples)\n",
    "# ------------------------------\n",
    "def sample_interior(N: int, dev):\n",
    "    # Uniform interior in (0,1)^2\n",
    "    x = torch.rand(N, 1, device=dev, dtype=DTYPE)\n",
    "    y = torch.rand(N, 1, device=dev, dtype=DTYPE)\n",
    "    return x, y\n",
    "\n",
    "def sample_boundaries(N_edge: int, dev):\n",
    "    y0 = torch.rand(N_edge, 1, device=dev, dtype=DTYPE)\n",
    "    y1 = torch.rand(N_edge, 1, device=dev, dtype=DTYPE)\n",
    "    x0 = torch.rand(N_edge, 1, device=dev, dtype=DTYPE)\n",
    "    x1 = torch.rand(N_edge, 1, device=dev, dtype=DTYPE)\n",
    "\n",
    "    # Edges:\n",
    "    xb0 = torch.zeros_like(y0); yb0 = y0                   # x=0, u=0\n",
    "    xb1 = torch.ones_like(y1);  yb1 = y1                   # x=1, u=sin(pi*y)\n",
    "    yb2 = torch.zeros_like(x0); xb2 = x0                   # y=0, u=0\n",
    "    yb3 = torch.ones_like(x1);  xb3 = x1                   # y=1, u=0\n",
    "\n",
    "    # Targets on boundaries:\n",
    "    u_b0 = torch.zeros_like(y0)                            # u(0,y)=0\n",
    "    u_b1 = torch.sin(math.pi * y1)                         # u(1,y)=sin(pi*y)\n",
    "    u_b2 = torch.zeros_like(x0)                            # u(x,0)=0\n",
    "    u_b3 = torch.zeros_like(x1)                            # u(x,1)=0\n",
    "\n",
    "    # Stack each edge into batches\n",
    "    Bx = [xb0, xb1, xb2, xb3]\n",
    "    By = [yb0, yb1, yb2, yb3]\n",
    "    Bu = [u_b0, u_b1, u_b2, u_b3]\n",
    "    return Bx, By, Bu\n",
    "\n",
    "def bc_loss(model: MLP, Bx: List[torch.Tensor], By: List[torch.Tensor], Bu: List[torch.Tensor]) -> torch.Tensor:\n",
    "    mse = nn.MSELoss()\n",
    "    total = 0.0\n",
    "    for xb, yb, ub in zip(Bx, By, Bu):\n",
    "        z = torch.cat([xb, yb], dim=1)\n",
    "        u_pred, _, _ = model(z)\n",
    "        total = total + mse(u_pred, ub)\n",
    "    return total\n",
    "\n",
    "# ------------------------------\n",
    "# Memory measurement helpers\n",
    "# ------------------------------\n",
    "class PeakMem:\n",
    "    def __init__(self, label: str):\n",
    "        self.label = label\n",
    "        self.cuda = on_cuda()\n",
    "        self.peak_bytes = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if self.cuda:\n",
    "            torch.cuda.synchronize()\n",
    "            self.peak_bytes = torch.cuda.max_memory_allocated()\n",
    "            print(f\"[{self.label}] Peak CUDA allocated: {bytes_to_mb(self.peak_bytes):.3f} MB\")\n",
    "        else:\n",
    "            # CPU mode: report rough param + tensor allocations (not perfect)\n",
    "            print(f\"[{self.label}] CPU mode: use theoretical estimates above; \"\n",
    "                  f\"PyTorch doesn't expose 'graph-only' CPU memory precisely.\")\n",
    "\n",
    "# ------------------------------\n",
    "# One training step for each method\n",
    "# ------------------------------\n",
    "def train_step_manual(model: MLP, opt, x_pde, y_pde, Bx, By, Bu):\n",
    "    model.train()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    z0 = torch.cat([x_pde, y_pde], dim=1)  # (N,2)\n",
    "    u, uxx, uyy = manual_seconds(model, z0)  # (N,1) each\n",
    "\n",
    "    # PDE residual: u_xx + u_yy = 0\n",
    "    r = uxx + uyy\n",
    "    loss_pde = (r**2).mean()\n",
    "\n",
    "    # Boundary conditions\n",
    "    loss_bc = bc_loss(model, Bx, By, Bu)\n",
    "\n",
    "    loss = loss_pde + loss_bc\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return float(loss.item())\n",
    "\n",
    "def train_step_autograd(model: MLP, opt, x_pde, y_pde, Bx, By, Bu):\n",
    "    model.train()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Need grads on inputs for autograd.grad\n",
    "    x = x_pde.clone().detach().requires_grad_(True)\n",
    "    y = y_pde.clone().detach().requires_grad_(True)\n",
    "\n",
    "    u, uxx, uyy = autograd_seconds(model, x, y)\n",
    "\n",
    "    # PDE residual: u_xx + u_yy = 0\n",
    "    r = uxx + uyy\n",
    "    loss_pde = (r**2).mean()\n",
    "\n",
    "    # Boundary conditions\n",
    "    loss_bc = bc_loss(model, Bx, By, Bu)\n",
    "\n",
    "    loss = loss_pde + loss_bc\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return float(loss.item())\n",
    "\n",
    "# ------------------------------\n",
    "# Theoretical estimator (activations only)\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class EstimationConfig:\n",
    "    N: int\n",
    "    n0: int\n",
    "    hidden: List[int]\n",
    "    k_coords: int = 2\n",
    "    dtype_bytes: int = 4  # fp32\n",
    "\n",
    "def estimate_activation_bytes_forward_mode(cfg: EstimationConfig, baseline: str = \"upper\",\n",
    "                                           need_first: bool = True, need_pure_second: bool = True,\n",
    "                                           include_mixed: bool = False) -> int:\n",
    "    S = sum(cfg.hidden)\n",
    "    n0 = cfg.n0\n",
    "    N = cfg.N\n",
    "    k = cfg.k_coords\n",
    "    b = cfg.dtype_bytes\n",
    "\n",
    "    if baseline == \"lower\":\n",
    "        base_elts = N * (n0 + S)                 # Z0 + one of (L/Z) per layer\n",
    "    else:\n",
    "        base_elts = N * (n0 + 2*S)               # Z0 + both (L and Z) per layer\n",
    "\n",
    "    extra = 0\n",
    "    if need_first:\n",
    "        extra += N * (2 * k * S)                 # H,G per layer per coord\n",
    "    if need_pure_second:\n",
    "        extra += N * (2 * k * S)                 # F,E per layer per coord\n",
    "    if include_mixed and k >= 2:\n",
    "        extra += N * (2 * (k*(k-1)//2) * S)      # K,J per layer per pair\n",
    "\n",
    "    return (base_elts + extra) * b\n",
    "\n",
    "def estimate_activation_bytes_autograd(cfg: EstimationConfig, baseline: str = \"upper\",\n",
    "                                       need_first: bool = True, need_pure_second: bool = True) -> Tuple[int,int]:\n",
    "    \"\"\"\n",
    "    Return (low_bytes, up_bytes) for autograd.grad.\n",
    "    Low ~ 1 activation per layer per coord per grad graph; Up ~ 2 (like H/G pair).\n",
    "    \"\"\"\n",
    "    S = sum(cfg.hidden)\n",
    "    n0 = cfg.n0\n",
    "    N = cfg.N\n",
    "    k = cfg.k_coords\n",
    "    b = cfg.dtype_bytes\n",
    "\n",
    "    if baseline == \"lower\":\n",
    "        base_elts = N * (n0 + S)\n",
    "    else:\n",
    "        base_elts = N * (n0 + 2*S)\n",
    "\n",
    "    low_extra = 0\n",
    "    up_extra  = 0\n",
    "    if need_first:\n",
    "        low_extra += N * (1 * k * S)\n",
    "        up_extra  += N * (2 * k * S)\n",
    "    if need_pure_second:\n",
    "        low_extra += N * (1 * k * S)\n",
    "        up_extra  += N * (2 * k * S)\n",
    "    # (Mixed terms would add more; Laplace uses only pure seconds.)\n",
    "\n",
    "    return (base_elts + low_extra) * b, (base_elts + up_extra) * b\n",
    "\n",
    "# ------------------------------\n",
    "# Main demo\n",
    "# ------------------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    dev = device()\n",
    "    print(f\"Device: {dev.type.upper()}\")\n",
    "\n",
    "    # Data\n",
    "    x_pde, y_pde = sample_interior(N_PDE, dev)\n",
    "    Bx, By, Bu = sample_boundaries(N_BC_EDGE, dev)\n",
    "\n",
    "    # Theoretical estimates (activations/graph stash only)\n",
    "    cfg = EstimationConfig(N=N_PDE, n0=2, hidden=HIDDEN, k_coords=2, dtype_bytes=4)\n",
    "    fwd_bytes = estimate_activation_bytes_forward_mode(cfg, baseline=\"upper\",\n",
    "                                                       need_first=True, need_pure_second=True, include_mixed=False)\n",
    "    ag_low, ag_up = estimate_activation_bytes_autograd(cfg, baseline=\"upper\",\n",
    "                                                       need_first=True, need_pure_second=True)\n",
    "    print(\"\\n[Theoretical graph/activation stash (fp32)]\")\n",
    "    print(f\"Forward-mode (manual, with u_xx & u_yy): ~{bytes_to_mb(fwd_bytes):.3f} MB\")\n",
    "    print(f\"autograd.grad  (low..up bounds):        ~{bytes_to_mb(ag_low):.3f} .. {bytes_to_mb(ag_up):.3f} MB\")\n",
    "\n",
    "    # Empirical peak memory per method (one training step)\n",
    "    # Fresh model/opt each to avoid cross-graph interference\n",
    "    model_A = MLP(2, HIDDEN, 1).to(dev).to(DTYPE)\n",
    "    opt_A = torch.optim.Adam(model_A.parameters(), lr=LR)\n",
    "\n",
    "    with PeakMem(\"Manual forward-mode (u_xx, u_yy)\"):\n",
    "        lossA = train_step_manual(model_A, opt_A, x_pde, y_pde, Bx, By, Bu)\n",
    "\n",
    "    model_B = MLP(2, HIDDEN, 1).to(dev).to(DTYPE)\n",
    "    opt_B = torch.optim.Adam(model_B.parameters(), lr=LR)\n",
    "\n",
    "    with PeakMem(\"autograd.grad (u_xx, u_yy)\"):\n",
    "        lossB = train_step_autograd(model_B, opt_B, x_pde, y_pde, Bx, By, Bu)\n",
    "\n",
    "    print(f\"\\nLoss (manual):   {lossA:.6f}\")\n",
    "    print(f\"Loss (autograd): {lossB:.6f}\")\n",
    "    print(\"\\nNote: CUDA numbers are peak *allocated* VRAM in the step,\")\n",
    "    print(\"      dominated by 'saved for backward' activations + grad graphs.\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
