{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a86bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim     \n",
    "\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de97fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "\n",
    "        self.layers = layers\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.H1 = self.linears[0]\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(self.layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "    def forward_direct(self, x):\n",
    "        \n",
    "        z = x.float()\n",
    "        H = self.linears[0].weight\n",
    "\n",
    "        for i in range(len(self.layers)-2):\n",
    "            L = self.linears[i](z)\n",
    "            z = self.activation(L)\n",
    "            G = (1-torch.square(z))*H.t() #\\sigma'(L)*H\n",
    "            H = torch.matmul(self.linears[i+1].weight,G.t())\n",
    "\n",
    "        z = self.linears[-1](z)\n",
    "         \n",
    "        return z,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68f501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = np.array([1,50,50,50,1])\n",
    "PINN = Sequentialmodel(layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eba226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value tensor([[-0.5295]], grad_fn=<AddmmBackward0>)\n",
      "Direct Calculation of Derivative tensor([[0.0003]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004698514938354492"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0).view(-1,1)\n",
    "\n",
    "start_time = time.time()\n",
    "z,H = PINN.forward_direct(x)\n",
    "# print(\"output \",z.shape,H.shape)\n",
    "print(\"Function value\",z)\n",
    "print(\"Direct Calculation of Derivative\",H)\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e009433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Calculation of Derivative tensor([[0.0003]], grad_fn=<TBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30113983154296875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x.requires_grad = True\n",
    "y = PINN(x)\n",
    "y_x = autograd.grad(y,x,torch.ones([x.shape[0], 1]), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "print(\"Autograd Calculation of Derivative\",y_x)\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69f7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "t_train = torch.linspace(0, 5, 100).view(-1, 1)\n",
    "u_train = (0.5 * (t_train **2) + t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f11959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Collocation Points\n",
    "\n",
    "N_f = 10000\n",
    "t_f = torch.linspace(0,5,N_f).view(-1,1).requires_grad_(True)\n",
    "\n",
    "\n",
    "# Generate IC Points\n",
    "\n",
    "t_i = torch.tensor([[0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e180133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residuals_with_autograd():\n",
    "    u_f_pred = PINN(t_f)\n",
    "    u_t = autograd.grad(u_f_pred,t_f,grad_outputs=torch.ones_like(u_f_pred), create_graph=True)[0]\n",
    "    \n",
    "    res_pde = u_t - t_f\n",
    "\n",
    "\n",
    "    u_i_pred = PINN(t_i)\n",
    "\n",
    "    res_ic = u_i_pred\n",
    "    \n",
    "\n",
    "    return res_pde, res_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68914be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_residuals_directly():\n",
    "#     z,u_t = PINN.forward_direct(t_f)\n",
    "    \n",
    "    \n",
    "#     res_pde = u_t - t_f\n",
    "\n",
    "\n",
    "#     u_i_pred = PINN(t_i)\n",
    "\n",
    "#     res_ic = u_i_pred\n",
    "    \n",
    "\n",
    "#     return res_pde, res_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371f1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses_autograd():\n",
    "    res_pde, res_ic = compute_residuals_with_autograd()\n",
    "\n",
    "    loss_pde = torch.mean(res_pde**2)\n",
    "    loss_ic = torch.mean(res_ic **2)\n",
    "\n",
    "    u_pred = PINN(t_train)\n",
    "\n",
    "    loss_data = torch.mean((u_pred - u_train)**2)\n",
    "\n",
    "    total_loss = loss_pde + loss_ic + loss_data\n",
    "\n",
    "    return loss_pde, loss_ic,loss_data, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_losses_direct():\n",
    "#     res_pde, res_ic = compute_residuals_directly()\n",
    "\n",
    "#     loss_pde = torch.mean(res_pde**2)\n",
    "#     loss_ic = torch.mean(res_ic **2)\n",
    "\n",
    "#     u_pred = PINN(t_train)\n",
    "\n",
    "#     loss_data = torch.mean((u_pred - u_train)**2)\n",
    "\n",
    "#     total_loss = loss_pde + loss_ic + loss_data\n",
    "\n",
    "#     return loss_pde, loss_ic,loss_data, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7c9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(PINN.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06db39b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss_pde=8.589e+00,  loss_ic=0.000e+00, loss_data=7.858e+01, total=8.716e+01\n",
      "Epoch 500: loss_pde=6.703e-01,  loss_ic=2.431e-02, loss_data=1.582e-01, total=8.529e-01\n",
      "Epoch 1000: loss_pde=6.447e-01,  loss_ic=2.653e-02, loss_data=1.046e-01, total=7.758e-01\n",
      "Epoch 1500: loss_pde=6.438e-01,  loss_ic=2.677e-02, loss_data=1.020e-01, total=7.725e-01\n",
      "Epoch 2000: loss_pde=6.437e-01,  loss_ic=2.685e-02, loss_data=1.013e-01, total=7.719e-01\n",
      "Epoch 2500: loss_pde=6.437e-01,  loss_ic=2.683e-02, loss_data=1.012e-01, total=7.718e-01\n",
      "Epoch 3000: loss_pde=6.437e-01,  loss_ic=2.680e-02, loss_data=1.012e-01, total=7.717e-01\n",
      "Epoch 3500: loss_pde=6.470e-01,  loss_ic=2.955e-02, loss_data=9.543e-02, total=7.720e-01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52251/774133324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss_pde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_ic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss_pde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52251/1275266966.py\u001b[0m in \u001b[0;36mcompute_losses_autograd\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_losses_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mres_pde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_residuals_with_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_pde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_pde\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_ic\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52251/1743638205.py\u001b[0m in \u001b[0;36mcompute_residuals_with_autograd\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_residuals_with_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mu_f_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mu_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_f_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_f_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mres_pde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "loss_pde_list, loss_ic_list, loss_data_list, total_loss_list = [],[],[],[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_pde, loss_ic, loss_data, total_loss = compute_losses_autograd()\n",
    "\n",
    "    loss_pde.backward(retain_graph=True)\n",
    "    loss_ic.backward(retain_graph=True)\n",
    "    loss_data.backward()\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_pde_list.append(loss_pde.item()); loss_ic_list.append(loss_ic.item()); loss_data_list.append(loss_data.item()); total_loss_list.append(total_loss.item())\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: loss_pde={loss_pde.item():.3e},  loss_ic={loss_ic.item():.3e}, loss_data={loss_data.item():.3e}, total={total_loss.item():.3e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Total Training Time: {(end_time - start_time): .4f}seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97404792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss_pde=2.837e+00,  loss_ic=3.054e-03, loss_data=5.106e-01, total=3.350e+00\n",
      "Epoch 500: loss_pde=2.718e+00,  loss_ic=8.119e-03, loss_data=5.883e-01, total=3.314e+00\n",
      "Epoch 1000: loss_pde=2.718e+00,  loss_ic=8.987e-03, loss_data=5.877e-01, total=3.314e+00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47444/1778702706.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss_pde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_ic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss_pde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mloss_ic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# num_epochs = 5000\n",
    "\n",
    "# loss_pde_list, loss_ic_list, loss_data_list, total_loss_list = [],[],[],[]\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     loss_pde, loss_ic, loss_data, total_loss = compute_losses_direct()\n",
    "\n",
    "#     loss_pde.backward(retain_graph=True)\n",
    "#     loss_ic.backward(retain_graph=True)\n",
    "#     loss_data.backward()\n",
    "\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     loss_pde_list.append(loss_pde.item()); loss_ic_list.append(loss_ic.item()); loss_data_list.append(loss_data.item()); total_loss_list.append(total_loss.item())\n",
    "\n",
    "#     if epoch % 500 == 0:\n",
    "#         print(f\"Epoch {epoch}: loss_pde={loss_pde.item():.3e},  loss_ic={loss_ic.item():.3e}, loss_data={loss_data.item():.3e}, total={total_loss.item():.3e}\")\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(f'Total Training Time: {(end_time - start_time): .4f}seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
