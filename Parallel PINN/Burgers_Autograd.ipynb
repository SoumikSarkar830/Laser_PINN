{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75a4762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "107306b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "\n",
    "        self.layers = layers\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.H1 = self.linears[0]\n",
    "\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,x,t):              \n",
    "        \n",
    "        # for i in range(len(self.layers)-2):\n",
    "        #     z = self.linears[i](a)\n",
    "        #     a = self.activation(z)\n",
    "\n",
    "        a = torch.cat([x,t], dim = 1)    #(10000,2)\n",
    "\n",
    "        for i in range(len(self.layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "\n",
    "\n",
    "            \n",
    "        b = self.linears[-1](a) \n",
    "         \n",
    "        return b\n",
    "    \n",
    "    def forward_direct(self, x):\n",
    "        \n",
    "        z = x.float()\n",
    "        H = self.linears[0].weight\n",
    "\n",
    "        for i in range(len(self.layers)-2):\n",
    "            L = self.linears[i](z)\n",
    "            z = self.activation(L)\n",
    "            G = (1-torch.square(z))*H.t() #\\sigma'(L)*H\n",
    "            H = torch.matmul(self.linears[i+1].weight,G.t())\n",
    "\n",
    "        z = self.linears[-1](z)\n",
    "         \n",
    "        return z,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77edc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = np.array([2,50,50,50,1])\n",
    "PINN = Sequentialmodel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbddcbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "\n",
    "x = torch.linspace(-1,1,100, requires_grad = True).view(-1,1)\n",
    "t = torch.linspace(0,1,100, requires_grad = True).view(-1,1)\n",
    "\n",
    "\n",
    "if torch.is_tensor(x) != True:         \n",
    "    x = torch.from_numpy(x)  \n",
    "if torch.is_tensor(t) != True:         \n",
    "    t = torch.from_numpy(t) \n",
    "\n",
    "#convert to float\n",
    "x = x.float()\n",
    "t = t.float()\n",
    "\n",
    "    \n",
    "x_train,t_train = torch.meshgrid(x.squeeze(),t.squeeze(), indexing = 'xy')\n",
    "x_train = x_train.reshape(-1,1).requires_grad_(True)\n",
    "t_train = t_train.reshape(-1,1).requires_grad_(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79ad5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_residual(x, t, nu):\n",
    "    u = PINN(x,t)\n",
    "\n",
    "    du_dx = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    du_dt = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "    du_dx_x = torch.autograd.grad(du_dx, x, torch.ones_like(du_dx), create_graph=True)[0]\n",
    "\n",
    "    res_pde = du_dt - (nu / (np.pi)) * du_dx_x + u * du_dx\n",
    "\n",
    "    return res_pde\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0acfb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initial_condition(x):\n",
    "  u_ic = PINN(x, torch.zeros_like(x))\n",
    "  res_ic = u_ic - (-torch.sin(np.pi * x))\n",
    "  return res_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33de6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_condition(t):\n",
    "    u_left = PINN(torch.full_like(t, -1), t)\n",
    "    u_right = PINN(torch.full_like(t, 1), t)\n",
    "\n",
    "    res_left = u_left - torch.zeros_like(t)\n",
    "    res_right = u_right - torch.zeros_like(t)\n",
    "\n",
    "    return res_left, res_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cde8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses():\n",
    "   res_pde = pde_residual(x_train, t_train, nu = 0.01) \n",
    "   res_ic = initial_condition(x_train)\n",
    "   res_left, res_right = boundary_condition(t_train)\n",
    "\n",
    "   loss_pde = torch.mean(res_pde**2)\n",
    "   loss_ic = torch.mean(res_ic**2)\n",
    "   loss_bc = torch.mean(res_left**2) + torch.mean(res_right**2)\n",
    "\n",
    "   total_loss = loss_pde + loss_ic + loss_bc\n",
    "\n",
    "   return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85f5210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(PINN.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d9769c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.05367458239197731\n",
      "Epoch 500, Loss: 0.05935884639620781\n",
      "Epoch 1000, Loss: 0.06171201169490814\n",
      "Epoch 1500, Loss: 0.04555651918053627\n",
      "Epoch 2000, Loss: 0.043755561113357544\n",
      "Epoch 2500, Loss: 0.021487591788172722\n",
      "Epoch 3000, Loss: 0.10012179613113403\n",
      "Epoch 3500, Loss: 0.08944450318813324\n",
      "Epoch 4000, Loss: 0.059110794216394424\n",
      "Epoch 4500, Loss: 0.09762782603502274\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_loss = compute_losses()\n",
    "\n",
    "    \n",
    "    total_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch) % 500 == 0:\n",
    "     print(f'Epoch {epoch}, Loss: {total_loss.item()}')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
