{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e36531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim     \n",
    "\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54e27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "\n",
    "        self.layers = layers\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.H1 = self.linears[0]\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(self.layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "    \n",
    "    def forward_direct(self, x):\n",
    "        \n",
    "        z = x.float()\n",
    "        H = self.linears[0].weight\n",
    "\n",
    "        for i in range(len(self.layers)-2):\n",
    "            L = self.linears[i](z)\n",
    "            z = self.activation(L)\n",
    "            G = (1-torch.square(z))*H.t() #\\sigma'(L)*H\n",
    "            H = torch.matmul(self.linears[i+1].weight,G.t())\n",
    "\n",
    "        z = self.linears[-1](z)\n",
    "         \n",
    "        return z,H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e65a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = np.array([1,50,50,50,1])\n",
    "PINN = Sequentialmodel(layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a630c658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function value tensor([[-0.1490]], grad_fn=<AddmmBackward0>)\n",
      "Direct Calculation of Derivative tensor([[-0.0190]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004547119140625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0).view(-1,1)\n",
    "\n",
    "start_time = time.time()\n",
    "z,H = PINN.forward_direct(x)\n",
    "# print(\"output \",z.shape,H.shape)\n",
    "print(\"Function value\",z)\n",
    "print(\"Direct Calculation of Derivative\",H)\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2872d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd Calculation of Derivative tensor([[-0.0190]], grad_fn=<TBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29602813720703125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x.requires_grad = True\n",
    "y = PINN(x)\n",
    "y_x = autograd.grad(y,x,torch.ones([x.shape[0], 1]), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "print(\"Autograd Calculation of Derivative\",y_x)\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185d478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "\n",
    "t_train = torch.linspace(0, 5, 100).view(-1, 1)\n",
    "u_train = (0.5 * (t_train **2) + t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404f60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Collocation Points\n",
    "\n",
    "N_f = 10000\n",
    "t_f = torch.linspace(0,5,N_f).view(-1,1).requires_grad_(True)\n",
    "\n",
    "\n",
    "# Generate IC Points\n",
    "\n",
    "t_i = torch.tensor([[0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb4b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residuals_directly():\n",
    "    z,u_t = PINN.forward_direct(t_f)\n",
    "    \n",
    "    \n",
    "    res_pde = u_t - t_f\n",
    "\n",
    "\n",
    "    u_i_pred = PINN(t_i)\n",
    "\n",
    "    res_ic = u_i_pred\n",
    "    \n",
    "\n",
    "    return res_pde, res_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7770cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses_direct():\n",
    "    res_pde, res_ic = compute_residuals_directly()\n",
    "\n",
    "    loss_pde = torch.mean(res_pde**2)\n",
    "    loss_ic = torch.mean(res_ic **2)\n",
    "\n",
    "    u_pred = PINN(t_train)\n",
    "\n",
    "    loss_data = torch.mean((u_pred - u_train)**2)\n",
    "\n",
    "    total_loss = loss_pde + loss_ic + loss_data\n",
    "\n",
    "    return loss_pde, loss_ic,loss_data, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b34bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(PINN.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64aa516",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "loss_pde_list, loss_ic_list, loss_data_list, total_loss_list = [],[],[],[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_pde, loss_ic, loss_data, total_loss = compute_losses_direct()\n",
    "\n",
    "    loss_pde.backward(retain_graph=True)\n",
    "    loss_ic.backward(retain_graph=True)\n",
    "    loss_data.backward()\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_pde_list.append(loss_pde.item()); loss_ic_list.append(loss_ic.item()); loss_data_list.append(loss_data.item()); total_loss_list.append(total_loss.item())\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: loss_pde={loss_pde.item():.3e},  loss_ic={loss_ic.item():.3e}, loss_data={loss_data.item():.3e}, total={total_loss.item():.3e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'Total Training Time: {(end_time - start_time): .4f}seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
