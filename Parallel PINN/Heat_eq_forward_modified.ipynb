{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d82ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ca6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16254349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97794136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "\n",
    "        self.layers = layers\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.H1 = self.linears[0]\n",
    "\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,x,y,t):              \n",
    "        \n",
    "        # for i in range(len(self.layers)-2):\n",
    "        #     z = self.linears[i](a)\n",
    "        #     a = self.activation(z)\n",
    "\n",
    "        a = torch.cat([x,y,t], dim = 1)    #(N,3)\n",
    "\n",
    "        for i in range(len(self.layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "\n",
    "\n",
    "            \n",
    "        b = self.linears[-1](a) \n",
    "         \n",
    "        return b\n",
    "    \n",
    "    def forward_direct(self, x,y,t,keep=(\"x\",\"y\",\"t\")):\n",
    "        \n",
    "        z = torch.cat([x,y,t], dim = 1)    #(N,3)  \n",
    "\n",
    "        dic = {\"x\":0, \"y\":1, \"t\":2}  \n",
    "        idx = [dic[k] for k in keep]\n",
    "\n",
    "        N = z.shape[0]   \n",
    "\n",
    "\n",
    "        W1 = self.linears[0].weight    # (20,3)\n",
    "\n",
    "        H = W1[:, idx].unsqueeze(0)      # (1,20,3)\n",
    "        F = torch.zeros_like(H)          # (1,20,3)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(len(self.layers)-2):\n",
    "            L = self.linears[i](z)            # (N,20)\n",
    "            z = self.activation(L)            # (N,20)\n",
    "\n",
    "            z_1 = 1.0-torch.square(z)         # (N,20)\n",
    "            z_2 = -2*z + 2*torch.pow(z,3)     # (N,20)\n",
    "\n",
    "            z_1 = z_1.unsqueeze(-1)           # (N,20,1)\n",
    "            z_2 = z_2.unsqueeze(-1)           # (N,20,1)\n",
    "\n",
    "            if H.shape[0] == 1:\n",
    "                H = H.expand(N, -1, -1)                    # (N,20,3)\n",
    "                F = F.expand(N, -1, -1)                    # (N,20,3)\n",
    "\n",
    "            G = z_1*H    #\\sigma'(L)*H   # (N,20,3)\n",
    "            \n",
    " \n",
    "            C = z_2*H*H                  # (N,20,3)\n",
    "\n",
    "\n",
    "            E = C + z_1*F                # (N,20,3)\n",
    "\n",
    "            W_next = self.linears[i+1].weight.t()       # (20,20)\n",
    "\n",
    "            H = torch.einsum('nhk,hp->npk', G, W_next)  # (N,20,3)\n",
    "            F = torch.einsum('nhk,hp->npk', E, W_next)  # (N,20,3)\n",
    "\n",
    "\n",
    "        z = self.linears[-1](z)                         # (N,1)\n",
    "         \n",
    "        # return z, H_x, H_y, H_t, F_x, F_y, F_t\n",
    "        return z, H, F                        # H - (N,1,3), F - (N,1,3)\n",
    "\n",
    "\n",
    "    # # TENSORIZED\n",
    "    \n",
    "    \n",
    "    # def forward_direct(self, x,t):\n",
    "        \n",
    "    #     z = torch.cat([x,t], dim = 1)    #(N,2)      N = 90000\n",
    "    #     N = z.size(0)\n",
    "\n",
    "    #     W1 = self.linears[0].weight    # (20,2)\n",
    "\n",
    "    #     H = W1.unsqueeze(0).expand(N,-1,-1).contiguous()     #(N,20,2)\n",
    "    #     F = torch.zeros_like(H)\n",
    "\n",
    "    #     # H_x = W1[:, 0].unsqueeze(0)      # (1,20)\n",
    "    #     # F_x = torch.zeros_like(H_x)\n",
    "\n",
    "    #     # H_t = W1[:, 1].unsqueeze(0)      # (1,20)\n",
    "    #     # F_t = torch.zeros_like(H_t)\n",
    "        \n",
    "\n",
    "    #     for i in range(len(self.layers)-2):\n",
    "    #         L = self.linears[i](z)            # (N,20)\n",
    "    #         z = self.activation(L)            # (N,20)\n",
    "    #         z_1 = 1.0-torch.square(z)         # (N,20)\n",
    "    #         z_1 = z_1.unsqueeze(-1)           # (N,20,1)\n",
    "\n",
    "    #         G = z_1*H     #\\sigma'(L)*H   # (N,20,2)\n",
    "\n",
    "    #         # G_x = z_1*H_x     #\\sigma'(L)*H   # (N,20)\n",
    "    #         # G_t = z_1*H_t                     # (N,20)\n",
    "\n",
    "    #         z_2 = (-2*z + 2*torch.pow(z,3))   #(N,20)\n",
    "    #         z_2 = z_2.unsqueeze(-1)           # (N,20,1)\n",
    "\n",
    "    #         C = z_2*H*H                       # (N,20,2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #         # C_x = (-2*z + 2*torch.pow(z,3))*H_x*H_x\n",
    "    #         # C_t = (-2*z + 2*torch.pow(z,3))*H_t*H_t\n",
    "\n",
    "    #         E = C + z_1*F                     # (N,20,2)     \n",
    "\n",
    "    #         # E_x = C_x + z_1*F_x    # (N,50)\n",
    "    #         # E_t = C_t + z_1*F_t    # (N,50)\n",
    "\n",
    "    #         H = torch.matmul(G.transpose(1, 2),self.linears[i+1].weight.t()).transpose(1, 2)     # (N,2,20) * (20,1)  = (N,2,1).t() = (N,1,2)\n",
    "    #         F = torch.matmul(E.transpose(1, 2),self.linears[i+1].weight.t()).transpose(1, 2)     # (N,1,2)\n",
    "\n",
    "\n",
    "    #         # H_x = torch.matmul(G_x,self.linears[i+1].weight.t())\n",
    "    #         # F_x = torch.matmul(E_x,self.linears[i+1].weight.t())\n",
    "\n",
    "    #         # H_t = torch.matmul(G_t,self.linears[i+1].weight.t())\n",
    "    #         # F_t = torch.matmul(E_t,self.linears[i+1].weight.t())\n",
    "\n",
    "    #     z = self.linears[-1](z)\n",
    "\n",
    "    #     H_x = H[:, 0, 0].unsqueeze(-1)   # (N,1)\n",
    "    #     H_t = H[:, 0, 1].unsqueeze(-1)   # (N,1)\n",
    "    #     F_x = F[:, 0, 0].unsqueeze(-1)   # (N,1)\n",
    "    #     F_t = F[:, 0, 1].unsqueeze(-1)   # (N,1)\n",
    "         \n",
    "    #     return z, H_x, H_t, F_x, F_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb846975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = np.array([2,50,50,50,50,50,1])\n",
    "layers = np.array([3,20,20,20,20,20,1])\n",
    "# PINN = Sequentialmodel(layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting to ensure the reported peak truly reflects the training loop, rather than including earlier setup.\n",
    "\n",
    "# if device.type == 'cuda':\n",
    "#     torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529c2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "\n",
    "x = torch.linspace(0,1,150).view(-1,1)\n",
    "y = torch.linspace(0,1,150).view(-1,1)\n",
    "t = torch.linspace(0,5,150).view(-1,1)\n",
    "\n",
    "\n",
    "if torch.is_tensor(x) != True:         \n",
    "    x = torch.from_numpy(x)  \n",
    "if torch.is_tensor(y) != True:         \n",
    "    y = torch.from_numpy(y) \n",
    "if torch.is_tensor(t) != True:         \n",
    "    t = torch.from_numpy(t) \n",
    "\n",
    "#convert to float\n",
    "x = x.float()\n",
    "y = y.float()\n",
    "t = t.float()\n",
    "\n",
    "    \n",
    "x_train,y_train,t_train = torch.meshgrid(x.squeeze(),y.squeeze(),t.squeeze(), indexing = 'xy')\n",
    "x_train = x_train.reshape(-1,1).to(device)     \n",
    "y_train = y_train.reshape(-1,1).to(device) \n",
    "t_train = t_train.reshape(-1,1).to(device)     \n",
    "\n",
    "# x_train = x_train.reshape(-1,1)     \n",
    "# y_train = y_train.reshape(-1,1) \n",
    "# t_train = t_train.reshape(-1,1)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2721bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_residual(x,y,t, alpha):\n",
    "    u = PINN(x,y,t)\n",
    "\n",
    "    # z, H_x, H_y, H_t, F_x, F_y, F_t = PINN.forward_direct(x,y,t)\n",
    "    z, H, F = PINN.forward_direct(x,y,t, keep=(\"x\",\"y\",\"t\"))\n",
    "\n",
    "\n",
    "    du_dt = H['t']              # (N,1)\n",
    "    du_dx_x = F['x']            # (N,1)\n",
    "    du_dy_y = F['y']            # (N,1)\n",
    "    # du_dt_t = F_t\n",
    "\n",
    "    res_pde = du_dt - alpha * (du_dx_x + du_dy_y)\n",
    "\n",
    "\n",
    "    return res_pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20aa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition(x,y):\n",
    "  u_ic = PINN(x, y, torch.zeros_like(x))\n",
    "  res_ic = u_ic - ((torch.sin(np.pi * x))*(torch.sin(np.pi * y)))\n",
    "  return res_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e73b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_condition(x,y,t):\n",
    "    u_left = PINN(torch.full_like(t, 0),y, t)\n",
    "    u_right = PINN(torch.full_like(t, 1),y, t)\n",
    "\n",
    "    u_bottom = PINN(x,torch.full_like(t, 0), t)\n",
    "    u_top = PINN(x,torch.full_like(t, 1), t)\n",
    "\n",
    "    res_left = u_left - torch.zeros_like(t)\n",
    "    res_right = u_right - torch.zeros_like(t)\n",
    "    res_bottom = u_bottom - torch.zeros_like(t)\n",
    "    res_top = u_top - torch.zeros_like(t)\n",
    "\n",
    "    return res_left, res_right,res_bottom, res_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e8c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses():\n",
    "   res_pde = pde_residual(x_train, y_train, t_train, alpha = 0.01) \n",
    "   res_ic = initial_condition(x_train,y_train)\n",
    "   res_left, res_right,res_bottom, res_top = boundary_condition(x_train, y_train, t_train)\n",
    "\n",
    "   loss_pde = torch.mean(res_pde**2)\n",
    "   loss_ic = torch.mean(res_ic**2)\n",
    "   loss_bc = torch.mean(res_left**2) + torch.mean(res_right**2) + torch.mean(res_bottom**2) + torch.mean(res_top**2)\n",
    "\n",
    "   total_loss = loss_pde + loss_ic + loss_bc\n",
    "\n",
    "   return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2702f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(PINN.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd320160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of epochs\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# num_epochs = 10000\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     total_loss = compute_losses()\n",
    "\n",
    "    \n",
    "#     total_loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (epoch) % 200 == 0:\n",
    "#      print(f'Epoch {epoch}, Loss: {total_loss.item()}')\n",
    "\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(f'Total Training Time: {(end_time - start_time): .4f}seconds')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f74349",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "PINN = Sequentialmodel(layers).to(device)\n",
    "# PINN = Sequentialmodel(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df2c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(PINN.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68deda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Threshold loss as the stopping criteria\n",
    "\n",
    "# max_epochs = 15000\n",
    "# threshold = 0.002\n",
    "\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# ep = 0\n",
    "# while ep < max_epochs:\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     total_loss = compute_losses()\n",
    "\n",
    "    \n",
    "#     total_loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "\n",
    "#     if total_loss.item() < threshold:\n",
    "#         print(f\"Reached threshold loss {threshold} at epoch {ep}\")\n",
    "#         break\n",
    "\n",
    "#     if (ep) % 200 == 0:\n",
    "#      print(f'Epoch {ep}, Loss: {total_loss.item()}')\n",
    "\n",
    "#     ep += 1\n",
    "\n",
    "\n",
    "# print(f\"Training stopped at epoch {ep}, total time {time.time() - start_time:.2f} s\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b0ebbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 774.00 MiB. GPU 0 has a total capacity of 31.48 GiB of which 3.56 MiB is free. Process 218242 has 26.19 GiB memory in use. Including non-PyTorch memory, this process has 5.26 GiB memory in use. Of the allocated memory 4.66 GiB is allocated by PyTorch, and 243.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ep \u001b[38;5;241m<\u001b[39m max_outer_steps:\n\u001b[0;32m---> 22\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReached threshold loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at outer step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyro_env/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyro_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyro_env/lib/python3.10/site-packages/torch/optim/lbfgs.py:330\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    327\u001b[0m state\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    332\u001b[0m current_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyro_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m():\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mcompute_losses\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_losses\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m    res_pde \u001b[38;5;241m=\u001b[39m \u001b[43mpde_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m    res_ic \u001b[38;5;241m=\u001b[39m initial_condition(x_train,y_train)\n\u001b[1;32m      4\u001b[0m    res_left, res_right,res_bottom, res_top \u001b[38;5;241m=\u001b[39m boundary_condition(x_train, y_train, t_train)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mpde_residual\u001b[0;34m(x, y, t, alpha)\u001b[0m\n\u001b[1;32m      2\u001b[0m u \u001b[38;5;241m=\u001b[39m PINN(x,y,t)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# z, H_x, H_y, H_t, F_x, F_y, F_t = PINN.forward_direct(x,y,t)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m z, H, F \u001b[38;5;241m=\u001b[39m \u001b[43mPINN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m du_dt \u001b[38;5;241m=\u001b[39m H[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m]              \u001b[38;5;66;03m# (N,1)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m du_dx_x \u001b[38;5;241m=\u001b[39m F[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]            \u001b[38;5;66;03m# (N,1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m, in \u001b[0;36mSequentialmodel.forward_direct\u001b[0;34m(self, x, y, t, keep)\u001b[0m\n\u001b[1;32m     75\u001b[0m G \u001b[38;5;241m=\u001b[39m z_1\u001b[38;5;241m*\u001b[39mH    \u001b[38;5;66;03m#\\sigma'(L)*H   # (N,20,3)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m C \u001b[38;5;241m=\u001b[39m z_2\u001b[38;5;241m*\u001b[39mH\u001b[38;5;241m*\u001b[39mH                  \u001b[38;5;66;03m# (N,20,3)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m E \u001b[38;5;241m=\u001b[39m C \u001b[38;5;241m+\u001b[39m \u001b[43mz_1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mF\u001b[49m                \u001b[38;5;66;03m# (N,20,3)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m W_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()       \u001b[38;5;66;03m# (20,20)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnhk,hp->npk\u001b[39m\u001b[38;5;124m'\u001b[39m, G, W_next)  \u001b[38;5;66;03m# (N,20,3)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 774.00 MiB. GPU 0 has a total capacity of 31.48 GiB of which 3.56 MiB is free. Process 218242 has 26.19 GiB memory in use. Including non-PyTorch memory, this process has 5.26 GiB memory in use. Of the allocated memory 4.66 GiB is allocated by PyTorch, and 243.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Using LBFGS\n",
    "\n",
    "optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05,max_iter=20,history_size=50,tolerance_grad=1e-9,tolerance_change=1e-9,line_search_fn='strong_wolfe')\n",
    "\n",
    "max_outer_steps = 15000\n",
    "threshold = 0.002\n",
    "\n",
    "start_time = time.time()\n",
    "ep = 0\n",
    "\n",
    "\n",
    "def closure():\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = compute_losses()\n",
    "    total_loss.backward()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "while ep < max_outer_steps:\n",
    "\n",
    "    total_loss = optimizer.step(closure)\n",
    "\n",
    "    if total_loss.item() < threshold:\n",
    "        print(f\"Reached threshold loss {threshold} at outer step {ep}\")\n",
    "        break\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(f'Outer {ep}, Loss: {total_loss.item()}')\n",
    "\n",
    "    ep += 1\n",
    "\n",
    "print(f\"Training stopped at outer step {ep}, total time {time.time() - start_time:.2f} s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage after training\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    peak_mem = torch.cuda.max_memory_allocated(device)\n",
    "    print(f'Peak GPU Memory Usage: {peak_mem / 1e6: .2f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
